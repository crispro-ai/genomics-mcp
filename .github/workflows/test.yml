name: Clinical Genomics Domain Testing

on:
  push:
    branches: [main, develop]
    paths:
      - "domains/**"
  pull_request:
    branches: [main]
  workflow_dispatch:
    inputs:
      domain:
        description: "Domain to test (e.g., clinical_genomics)"
        required: true
        default: "clinical_genomics"
      runs:
        description: "Number of test runs (Pass@K)"
        required: false
        type: number
        default: 3

jobs:
  validate-and-test:
    runs-on: ubuntu-latest
    timeout-minutes: 120  # 2 hours for full test suite
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          submodules: recursive
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: Install uv
        run: |
          curl -LsSf https://astral.sh/uv/install.sh | sh
          echo "$HOME/.cargo/bin" >> $GITHUB_PATH
      
      - name: Install dependencies
        run: |
          uv venv
          source .venv/bin/activate
          uv pip install -e ".[dev]"
      
      - name: Validate domain structure
        run: |
          source .venv/bin/activate
          python -c "
          import json
          import yaml
          from pathlib import Path
          
          domain = '${{ inputs.domain || 'clinical_genomics' }}'
          domain_path = Path('domains') / domain
          
          # Check structure
          assert (domain_path / 'config.yaml').exists(), 'config.yaml missing'
          assert (domain_path / 'evaluators' / '__init__.py').exists(), 'evaluators/__init__.py missing'
          assert (domain_path / 'evaluators' / 'functions.py').exists(), 'evaluators/functions.py missing'
          assert (domain_path / 'tasks').exists(), 'tasks/ directory missing'
          
          # Validate config
          with open(domain_path / 'config.yaml') as f:
              config = yaml.safe_load(f)
              assert 'tasks' in config, 'tasks list missing in config'
          
          # Validate task JSONs
          tasks_dir = domain_path / 'tasks'
          task_files = list(tasks_dir.glob('*.json'))
          assert len(task_files) > 0, 'No task files found'
          
          for task_file in task_files:
              with open(task_file) as f:
                  task = json.load(f)
                  assert 'question' in task, f'{task_file.name}: question missing'
                  assert 'evaluators' in task, f'{task_file.name}: evaluators missing'
          
          print(f'‚úÖ Domain structure valid: {len(task_files)} tasks found')
          "
      
      - name: Run domain tests
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          SERP_API_KEY: ${{ secrets.SERP_API_KEY }}
          # Add your genomics API keys here
          CLINVAR_API_KEY: ${{ secrets.CLINVAR_API_KEY }}
          ONCOKB_API_KEY: ${{ secrets.ONCOKB_API_KEY }}
          PHARMGKB_API_KEY: ${{ secrets.PHARMGKB_API_KEY }}
        run: |
          source .venv/bin/activate
          
          # Run tests with alignerr_mcp CLI (if you copied it)
          # OR run your custom test script
          python scripts/run_tests.py \
            --domain ${{ inputs.domain || 'clinical_genomics' }} \
            --runs ${{ inputs.runs || 3 }} \
            --output results/
      
      - name: Generate test report
        if: always()
        run: |
          source .venv/bin/activate
          python scripts/generate_report.py \
            --input results/ \
            --output test-report.html
      
      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-results-${{ github.run_id }}
          path: |
            results/
            test-report.html
      
      - name: Comment PR with results
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const results = JSON.parse(fs.readFileSync('results/summary.json', 'utf8'));
            
            const comment = `
            ## üß¨ Clinical Genomics Test Results
            
            **Domain:** ${results.domain}
            **Tasks:** ${results.total_tasks}
            **Pass@1:** ${results.pass_at_1}%
            **Pass@3:** ${results.pass_at_3}%
            
            ### Task Breakdown
            ${results.categories.map(cat => 
              \`- **\${cat.name}:** \${cat.passed}/\${cat.total} tasks passed\`
            ).join('\\n')}
            
            **[View detailed report](https://github.com/${{github.repository}}/actions/runs/${{github.run_id}})**
            `;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
      
      - name: Fail if Pass@1 < 30% or > 70%
        run: |
          source .venv/bin/activate
          python -c "
          import json
          with open('results/summary.json') as f:
              results = json.load(f)
              pass_at_1 = results['pass_at_1']
              
              if pass_at_1 < 30:
                  print(f'‚ùå Pass@1 too low: {pass_at_1}% (target: 30-70%)')
                  exit(1)
              elif pass_at_1 > 70:
                  print(f'‚ö†Ô∏è  Pass@1 too high: {pass_at_1}% (target: 30-70%) - tasks may be too easy')
                  exit(1)
              else:
                  print(f'‚úÖ Pass@1 in target range: {pass_at_1}%')
          "

